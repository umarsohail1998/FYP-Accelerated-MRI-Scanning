{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Model.ipynb","provenance":[],"collapsed_sections":["eJ1bAIDNELg7","1NIfzJe9vw4E","QLgSlwAuIF3-","f9giIwy1bd6r","vTL4yziQN8ta","OKUYlQQIPZZt","G3xaWCZtO52g"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"FOCbo8yYE4G1"},"source":["#### This notebook shows how to read the fastMRI dataset and apply some simple transformations to the data."]},{"cell_type":"code","metadata":{"id":"oFdWvR0Qg8Cc"},"source":["import os\n","os.chdir(\"/content/drive/MyDrive/Accelerated MRI Scanning/Repositories/Model\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I6MS7ZUvbIf8"},"source":["#***Import Libraries***"]},{"cell_type":"code","metadata":{"id":"9_8uADLbE4G5"},"source":["%matplotlib inline\n","import h5py\n","import numpy as np\n","from PIL import Image\n","from PIL import ImageFilter\n","import matplotlib\n","import random\n","import cv2\n","# from scipy.fft import fft, ifft\n","import numpy as np\n","from matplotlib import pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-VXO48AUFJla"},"source":["!pip install runstats"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o7axuWk2atWr"},"source":["import numpy as np\n","import torch\n","import os\n","from collections import OrderedDict\n","from torch.autograd import Variable\n","import torch.nn as nn\n","from torch.nn import init\n","from torch.nn import functional as F\n","import functools\n","import torchvision.models as models\n","from torch.utils.tensorboard import SummaryWriter\n","writer= SummaryWriter()\n","import logging"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mfISHRZNh5l6"},"source":["# loading fastmri modules\n","from fastMRI.data import transforms\n","from fastMRI.data.mri_data import SliceData\n","from fastMRI.common.evaluate import nmse, psnr, ssim\n","from fastMRI.common.subsample import MaskFunc\n","from torch.utils.data import DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sR7M-ruGEF4P"},"source":["#***Generator***"]},{"cell_type":"code","metadata":{"id":"EuhhHGhJYfZu"},"source":["class ResnetGenerator(nn.Module):\n","    def __init__(\n","            self, input_nc, output_nc, chans=64, norm_layer=nn.BatchNorm2d, use_dropout=False,\n","            n_blocks=6, gpu_ids=[], use_parallel=True, learn_residual=False, padding_type='reflect'):\n","        assert (n_blocks >= 0)\n","        super(ResnetGenerator, self).__init__()\n","        self.in_chans = input_nc\n","        self.out_chans = output_nc\n","        self.chans = ngf\n","        self.gpu_ids = gpu_ids\n","        self.use_parallel = use_parallel\n","        self.learn_residual = learn_residual\n","        drop_prob = 0.1\n","        num_pool_layers = 4\n","\n","        if type(norm_layer) == functools.partial:\n","            use_bias = norm_layer.func == nn.InstanceNorm2d\n","        else:\n","            use_bias = norm_layer == nn.InstanceNorm2d\n","\n","\n","        self.down_sample_layers = nn.ModuleList([ConvBlock(self.in_chans, chans, drop_prob)])\n","        ch = chans\n","        for _ in range(num_pool_layers - 1):\n","            self.down_sample_layers.append(ConvBlock(ch, ch * 2, drop_prob))\n","            ch *= 2\n","        \n","        model = []\n","        for i in range(6):\n","            model += [\n","                ResnetBlock(ch,ch*2, padding_type='reflect', norm_layer=norm_layer, use_dropout=True, use_bias=use_bias)\n","            ]\n","        \n","        self.resnet = nn.Sequential(*model)\n","        # print(self.resnet)\n","\n","\n","        self.conv = ConvBlock(ch, ch * 2, drop_prob)\n","\n","\n","        self.up_conv = nn.ModuleList()\n","        self.up_transpose_conv = nn.ModuleList()\n","        for _ in range(num_pool_layers - 1):\n","            self.up_transpose_conv.append(TransposeConvBlock(ch * 2, ch))\n","            self.up_conv.append(ConvBlock(ch * 2, ch, drop_prob))\n","            ch //= 2\n","\n","        self.up_transpose_conv.append(TransposeConvBlock(ch * 2, ch))\n","        self.up_conv.append(\n","            nn.Sequential(\n","                ConvBlock(ch * 2, ch, drop_prob),\n","                nn.Conv2d(ch, self.out_chans, kernel_size=1, stride=1),\n","            )\n","        )\n","\n","    def forward(self, input):\n","      stack = []\n","      output = input\n","\n","      # apply down-sampling layers\n","      for layer in self.down_sample_layers:\n","          output = layer(output)\n","          stack.append(output)\n","          output = F.avg_pool2d(output, kernel_size=2, stride=2, padding=0)\n","\n","      output = self.resnet(output)\n","      output = self.conv(output)\n","\n","      # apply up-sampling layers\n","      for transpose_conv, conv in zip(self.up_transpose_conv, self.up_conv):\n","          downsample_layer = stack.pop()\n","          output = transpose_conv(output)\n","\n","          # reflect pad on the right/botton if needed to handle odd input dimensions\n","          padding = [0, 0, 0, 0]\n","          if output.shape[-1] != downsample_layer.shape[-1]:\n","              padding[1] = 1  # padding right\n","          if output.shape[-2] != downsample_layer.shape[-2]:\n","              padding[3] = 1  # padding bottom\n","          if torch.sum(torch.tensor(padding)) != 0:\n","              output = F.pad(output, padding, \"reflect\")\n","          output = torch.cat([output, downsample_layer], dim=1)\n","          output = conv(output)\n","\n","      return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BQDs9-M9EBHz"},"source":["class ResnetBlock(nn.Module):\n","\n","\tdef __init__(self, in_chans, out_chans, padding_type, norm_layer, use_dropout, use_bias):\n","\t\tsuper(ResnetBlock, self).__init__()\n","\n","\t\tpadAndConv = {\n","\t\t\t'reflect': [\n","                nn.ReflectionPad2d(1),\n","                nn.Conv2d(in_chans, in_chans, kernel_size=3, bias=use_bias)],\n","\t\t\t'replicate': [\n","                nn.ReplicationPad2d(1),\n","                nn.Conv2d(in_chans, in_chans, kernel_size=3, bias=use_bias)],\n","\t\t\t'zero': [\n","                nn.Conv2d(in_chans, in_chans, kernel_size=3, padding=1, bias=use_bias )]\n","\t\t}\n","\n","\t\ttry:\n","\t\t\tblocks = padAndConv[padding_type] + [norm_layer(in_chans), nn.LeakyReLU(negative_slope=0.2, inplace=True)] + [nn.Dropout(0.5)] if use_dropout else [] + padAndConv[padding_type] + [ norm_layer(in_chans)]\n","\t\texcept:\n","\t\t\traise NotImplementedError('padding [%s] is not implemented' % padding_type)\n","\n","\t\tself.conv_block = nn.Sequential(*blocks)\n","\n","\n","\tdef forward(self, x):\n","# \t\tprint(f\"\\nx: {x.shape}, output: {(self.conv_block(x)).shape}\\n\")\n","\t\tout = x + self.conv_block(x)\n","\t\treturn out"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AouBgRThoIcW"},"source":["class ConvBlock(nn.Module):\n","    \"\"\"\n","    A Convolutional Block that consists of two convolution layers each followed by\n","    instance normalization, LeakyReLU activation and dropout.\n","    \"\"\"\n","\n","    def __init__(self, in_chans: int, out_chans: int, drop_prob: float):\n","        \"\"\"\n","        Args:\n","            in_chans: Number of channels in the input.\n","            out_chans: Number of channels in the output.\n","            drop_prob: Dropout probability.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.in_chans = in_chans\n","        self.out_chans = out_chans\n","        self.drop_prob = drop_prob\n","\n","        self.layers = nn.Sequential(\n","            nn.Conv2d(in_chans, out_chans, kernel_size=3, padding=1, bias=True),\n","            nn.InstanceNorm2d(out_chans),\n","            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n","            nn.Dropout2d(drop_prob),\n","            nn.Conv2d(out_chans, out_chans, kernel_size=3, padding=1, bias=True),\n","            nn.InstanceNorm2d(out_chans),\n","            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n","            nn.Dropout2d(drop_prob),\n","        )\n","\n","    def forward(self, image: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Args:\n","            image: Input 4D tensor of shape `(N, in_chans, H, W)`.\n","\n","        Returns:\n","            Output tensor of shape `(N, out_chans, H, W)`.\n","        \"\"\"\n","        return self.layers(image)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zADA3DygD7W8"},"source":["class TransposeConvBlock(nn.Module):\n","    \"\"\"\n","    A Transpose Convolutional Block that consists of one convolution transpose\n","    layers followed by instance normalization and LeakyReLU activation.\n","    \"\"\"\n","\n","    def __init__(self, in_chans: int, out_chans: int):\n","        \"\"\"\n","        Args:\n","            in_chans: Number of channels in the input.\n","            out_chans: Number of channels in the output.\n","        \"\"\"\n","        super().__init__()\n","\n","        self.in_chans = in_chans\n","        self.out_chans = out_chans \n","\n","        self.layers = nn.Sequential(\n","            nn.ConvTranspose2d(\n","                in_chans, out_chans, kernel_size=2, stride=2, bias=False\n","            ),\n","            nn.InstanceNorm2d(out_chans),\n","            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n","        )\n","\n","    def forward(self, image: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Args:\n","            image: Input 4D tensor of shape `(N, in_chans, H, W)`.\n","\n","        Returns:\n","            Output tensor of shape `(N, out_chans, H*2, W*2)`.\n","        \"\"\"\n","        # print(f\"\\nimageCOnvtrans: {image.shape}\")\n","        return self.layers(image)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eJ1bAIDNELg7"},"source":["#***Discriminator***"]},{"cell_type":"code","metadata":{"id":"ClT1KDJ6YtR8"},"source":["# Defines the PatchGAN discriminator with the specified arguments.\n","class NLayerDiscriminator(nn.Module):\n","    def __init__(self, input_nc, ndf=64, n_layers=10, norm_layer=nn.BatchNorm2d, use_sigmoid=False, gpu_ids=[],\n","                 use_parallel=True):\n","        super(NLayerDiscriminator, self).__init__()\n","        self.gpu_ids = gpu_ids\n","        self.use_parallel = use_parallel\n","\n","        if type(norm_layer) == functools.partial:\n","            use_bias = norm_layer.func == nn.InstanceNorm2d\n","        else:\n","            use_bias = norm_layer == nn.InstanceNorm2d\n","\n","        kw = 4\n","        padw = int(np.ceil((kw - 1) / 2))\n","        sequence = [\n","            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n","            nn.LeakyReLU(0.2, True)\n","        ]\n","\n","        nf_mult = 1\n","        nf_mult_prev = 1\n","        for n in range(1, n_layers):\n","            nf_mult_prev = nf_mult\n","            nf_mult = min(2 ** n, 8)\n","            sequence += [\n","                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n","                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n","                norm_layer(ndf * nf_mult),\n","                nn.LeakyReLU(0.2, True)\n","            ]\n","\n","        nf_mult_prev = nf_mult\n","        nf_mult = min(2 ** n_layers, 8)\n","        sequence += [\n","            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n","            norm_layer(ndf * nf_mult),\n","            nn.LeakyReLU(0.2, True)\n","        ]\n","\n","        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n","\n","        if use_sigmoid:\n","            sequence += [nn.Sigmoid()]\n","\n","        self.model = nn.Sequential(*sequence)\n","\n","    def forward(self, input):\n","        if len(self.gpu_ids) and isinstance(input.data, torch.cuda.FloatTensor) and self.use_parallel:\n","            return nn.parallel.data_parallel(self.model, input, self.gpu_ids)\n","        else:\n","            return self.model(input)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1NIfzJe9vw4E"},"source":["#***Image Pool***"]},{"cell_type":"code","metadata":{"id":"Y6g-ON6lBpkU"},"source":["class ImagePool():\n","    def __init__(self, pool_size):\n","        self.pool_size = pool_size\n","        if self.pool_size > 0:\n","            self.num_imgs = 0\n","            self.images = []\n","\n","    def query(self, images):\n","        if self.pool_size == 0:\n","            return images\n","        return_images = []\n","        for image in images.data:\n","            image = torch.unsqueeze(image, 0)\n","            if self.num_imgs < self.pool_size:\n","                self.num_imgs = self.num_imgs + 1\n","                self.images.append(image)\n","                return_images.append(image)\n","            else:\n","                p = random.uniform(0, 1)\n","                if p > 0.5:\n","                    random_id = random.randint(0, self.pool_size-1)\n","                    tmp = self.images[random_id].clone()\n","                    self.images[random_id] = image\n","                    return_images.append(tmp)\n","                else:\n","                    return_images.append(image)\n","        return_images = Variable(torch.cat(return_images, 0))\n","        return return_images\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QLgSlwAuIF3-"},"source":["#***Losses***\n"]},{"cell_type":"code","metadata":{"id":"-eW5XhYOAg0r"},"source":["# TODO: Operation check\n","def contextual_bilateral_loss(x: torch.Tensor,\n","                              y: torch.Tensor,\n","                              weight_sp: float = 0.1,\n","                              band_width: float = 1.,\n","                              loss_type: str = 'cosine'):\n","    \"\"\"\n","    Computes Contextual Bilateral (CoBi) Loss between x and y,\n","        proposed in https://arxiv.org/pdf/1905.05169.pdf.\n","    Parameters\n","    ---\n","    x : torch.Tensor\n","        features of shape (N, C, H, W).\n","    y : torch.Tensor\n","        features of shape (N, C, H, W).\n","    band_width : float, optional\n","        a band-width parameter used to convert distance to similarity.\n","        in the paper, this is described as :math:`h`.\n","    loss_type : str, optional\n","        a loss type to measure the distance between features.\n","        Note: `l1` and `l2` frequently raises OOM.\n","    Returns\n","    ---\n","    cx_loss : torch.Tensor\n","        contextual loss between x and y (Eq (1) in the paper).\n","    k_arg_max_NC : torch.Tensor\n","        indices to maximize similarity over channels.\n","    \"\"\"\n","    LOSS_TYPES = ['cosine', 'l1', 'l2']\n","    assert x.size() == y.size(), 'input tensor must have the same size.'\n","    # assert loss_type in LOSS_TYPES, f'select a loss type from {LOSS_TYPES}.'\n","\n","    # spatial loss\n","    grid = compute_meshgrid(x.shape).to(x.device)\n","    dist_raw = compute_l2_distance(grid, grid)\n","    dist_tilde = compute_relative_distance(dist_raw)\n","    cx_sp = compute_cx(dist_tilde, band_width)\n","\n","    # feature loss\n","    if loss_type == 'cosine':\n","        dist_raw = compute_cosine_distance(x, y)\n","    elif loss_type == 'l1':\n","        dist_raw = compute_l1_distance(x, y)\n","    elif loss_type == 'l2':\n","        dist_raw = compute_l2_distance(x, y)\n","    dist_tilde = compute_relative_distance(dist_raw)\n","    cx_feat = compute_cx(dist_tilde, band_width)\n","    # combined loss\n","    cx_combine = (1. - weight_sp) * cx_feat + weight_sp * cx_sp\n","    k_max_NC, _ = torch.max(cx_combine, dim=2, keepdim=True)\n","    cx = k_max_NC.mean(dim=1)\n","    cx_loss = torch.mean(-torch.log(cx + 1e-5))\n","    return cx_loss\n","\n","def compute_cx(dist_tilde, band_width):\n","    w = torch.exp((1 - dist_tilde) / band_width)  # Eq(3)\n","    cx = w / torch.sum(w, dim=2, keepdim=True)  # Eq(4)\n","    return cx\n","\n","def compute_relative_distance(dist_raw):\n","    dist_min, _ = torch.min(dist_raw, dim=2, keepdim=True)\n","    dist_tilde = dist_raw / (dist_min + 1e-5)\n","    return dist_tilde\n","\n","def compute_cosine_distance(x, y):\n","    # mean shifting by channel-wise mean of `y`.\n","    y_mu = y.mean(dim=(0, 2, 3), keepdim=True)\n","    x_centered = x - y_mu\n","    y_centered = y - y_mu\n","\n","    # L2 normalization\n","    x_normalized = F.normalize(x_centered, p=2, dim=1)\n","    y_normalized = F.normalize(y_centered, p=2, dim=1)\n","\n","    # channel-wise vectorization\n","    N, C, *_ = x.size()\n","    x_normalized = x_normalized.reshape(N, C, -1)  # (N, C, H*W)\n","    y_normalized = y_normalized.reshape(N, C, -1)  # (N, C, H*W)\n","\n","    # consine similarity\n","    cosine_sim = torch.bmm(x_normalized.transpose(1, 2),\n","                           y_normalized)  # (N, H*W, H*W)\n","    # convert to distance\n","    dist = 1 - cosine_sim\n","    return dist\n","\n","\n","# TODO: Considering avoiding OOM.\n","def compute_l1_distance(x: torch.Tensor, y: torch.Tensor):\n","    N, C, H, W = x.size()\n","    x_vec = x.view(N, C, -1)\n","    y_vec = y.view(N, C, -1)\n","    dist = x_vec.unsqueeze(2) - y_vec.unsqueeze(3)\n","    dist = dist.sum(dim=1).abs()\n","    dist = dist.transpose(1, 2).reshape(N, H*W, H*W)\n","    dist = dist.clamp(min=0.)\n","    return dist\n","\n","# TODO: Considering avoiding OOM.\n","def compute_l2_distance(x, y):\n","    N, C, H, W = x.size()\n","    x_vec = x.view(N, C, -1)\n","    y_vec = y.view(N, C, -1)\n","    x_s = torch.sum(x_vec ** 2, dim=1)\n","    y_s = torch.sum(y_vec ** 2, dim=1)\n","    A = y_vec.transpose(1, 2) @ x_vec\n","    dist = y_s - 2 * A + x_s.transpose(0, 1)\n","    dist = dist.transpose(1, 2).reshape(N, H*W, H*W)\n","    dist = dist.clamp(min=0.)\n","    return dist\n","\n","def compute_meshgrid(shape):\n","    N, C, H, W = shape\n","    rows = torch.arange(0, H, dtype=torch.float32) / (H + 1)\n","    cols = torch.arange(0, W, dtype=torch.float32) / (W + 1)\n","\n","    feature_grid = torch.meshgrid(rows, cols)\n","    feature_grid = torch.stack(feature_grid).unsqueeze(0)\n","    feature_grid = torch.cat([feature_grid for _ in range(N)], dim=0)\n","\n","    return feature_grid"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2xCHBqZtYvs8"},"source":["class ContentLoss:\n","\tdef __init__(self, loss):\n","\t\tself.criterion = loss\n","\t\t\t\n","\tdef get_loss(self, fakeIm, realIm):\n","\t\treturn self.criterion(fakeIm, realIm)\n","\n","class PerceptualLoss():\n","  def contentFunc(self):\n","    conv_3_3_layer = 14\n","    cnn = models.vgg19(pretrained=True).features\n","    cnn = cnn.cuda()\n","    model = nn.Sequential()\n","    model = model.cuda()\n","    for i,layer in enumerate(list(cnn)):\n","      model.add_module(str(i),layer)\n","      if i == conv_3_3_layer:\n","        break\n","    return model\n","\n","  def __init__(self, loss):\n","    self.criterion = loss\n","    self.contentFunc = self.contentFunc()\n","\n","  def get_loss(self, fakeIm, realIm):\n","    new_fakeIm = fakeIm[:,:, :, :] * torch.ones(3)[None,:, None, None].to(torch.device('cuda') )\n","    new_realIm = realIm[:,:, :, :] * torch.ones(3)[None,:, None, None].to(torch.device('cuda') )\n","\n","    f_fake = self.contentFunc.forward(new_fakeIm)\n","\n","    f_real = self.contentFunc.forward(new_realIm)\n","    f_real_no_grad = f_real\n","    loss = self.criterion(f_fake, f_real_no_grad)\n","    return loss\n","\n","class GANLoss(nn.Module):\n","\tdef __init__(\n","\t\t\tself, use_l1=True, target_real_label=1.0,\n","\t\t\ttarget_fake_label=0.0, tensor=torch.FloatTensor):\n","\t\tsuper(GANLoss, self).__init__()\n","\t\tself.real_label = target_real_label\n","\t\tself.fake_label = target_fake_label\n","\t\tself.real_label_var = None\n","\t\tself.fake_label_var = None\n","\t\tself.Tensor = tensor\n","\t\tif use_l1:\n","\t\t\tself.loss = nn.L1Loss()\n","\t\telse:\n","\t\t\tself.loss = nn.BCELoss()\n","\n","\tdef get_target_tensor(self, input, target_is_real):\n","\t\ttarget_tensor = None\n","\t\tif target_is_real:\n","\t\t\tcreate_label = ((self.real_label_var is None) or\n","\t\t\t\t\t\t\t(self.real_label_var.numel() != input.numel()))\n","\t\t\tif create_label:\n","\t\t\t\treal_tensor = self.Tensor(input.size()).fill_(self.real_label)\n","\t\t\t\tself.real_label_var = Variable(real_tensor, requires_grad=False)\n","\t\t\ttarget_tensor = self.real_label_var\n","\t\telse:\n","\t\t\tcreate_label = ((self.fake_label_var is None) or\n","\t\t\t\t\t\t\t(self.fake_label_var.numel() != input.numel()))\n","\t\t\tif create_label:\n","\t\t\t\tfake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n","\t\t\t\tself.fake_label_var = Variable(fake_tensor, requires_grad=False)\n","\t\t\ttarget_tensor = self.fake_label_var\n","\t\treturn target_tensor\n","\n","\tdef __call__(self, input, target_is_real):\n","\t\ttarget_tensor = self.get_target_tensor(input, target_is_real).to(torch.device('cuda'))\n","\t\treturn self.loss(input, target_tensor)\n","    \n","\n","class DiscLoss:\n","\tdef name(self):\n","\t\treturn 'DiscLoss'\n","\n","\tdef __init__(self):\n","\t\tself.criterionGAN = GANLoss(use_l1=False)\n","\t\tself.fake_AB_pool = ImagePool(pool_size=50)\n","\t\t\n","\tdef get_g_loss(self,net, realA, fakeB):\n","\t\t# First, G(A) should fake the discriminator\n","\t\tpred_fake = net.forward(fakeB)\n","\t\treturn self.criterionGAN(pred_fake, 1)\n","\t\t\n","\tdef get_loss(self, net, realA, fakeB, realB):\n","\t\t# Fake\n","\t\t# stop backprop to the generator by detaching fake_B\n","\t\t# Generated Image Disc Output should be close to zero\n","\t\tself.pred_fake = net.forward(fakeB)\n","\t\tself.loss_D_fake = self.criterionGAN(self.pred_fake, 0)\n","\n","\t\t# Real\n","\t\tself.pred_real = net.forward(realB)\n","\t\tself.loss_D_real = self.criterionGAN(self.pred_real, 1)\n","\n","\t\t# Combined loss\n","\t\tself.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n","\t\treturn self.loss_D\n","\t\t\n","# class DiscLossLS(DiscLoss):\n","# \tdef name(self):\n","# \t\treturn 'DiscLossLS'\n","\n","# \tdef __init__(self):\n","# \t\tsuper(DiscLoss, self).__init__()\n","# \t\t# DiscLoss.initialize(self, opt, tensor)\n","# \t\tself.criterionGAN = GANLoss(use_l1=True, tensor=tensor)\n","\t\t\n","# \tdef get_g_loss(self,net, realA, fakeB):\n","# \t\treturn DiscLoss.get_g_loss(self,net, realA, fakeB)\n","\t\t\n","# \tdef get_loss(self, net, realA, fakeB, realB):\n","# \t\treturn DiscLoss.get_loss(self, net, realA, fakeB, realB)\n","\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ikGD2rfnY0Vm"},"source":["\n","# class DiscLossWGANGP(DiscLossLS):\n","#     def name(self):\n","#       return 'DiscLossWGAN-GP'\n","\n","#     def __init__():\n","#       super(DiscLossWGANGP, self).__init__()\n","#       # DiscLossLS.initialize(self, opt, tensor)\n","#       self.LAMBDA = 10\n","        \n","#     def get_g_loss(net, realA, fakeB):\n","#         # First, G(A) should fake the discriminator\n","#         D_fake = net.forward(fakeB)\n","#         return -D_fake.mean()\n","\n","#     def calc_gradient_penalty(self, netD, real_data, fake_data):\n","#         alpha = torch.rand(1, 1)\n","#         alpha = alpha.expand(real_data.size())\n","#         alpha = alpha.cuda()\n","\n","#         interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n","\n","#         interpolates = interpolates.cuda()\n","#         interpolates = Variable(interpolates, requires_grad=True)\n","\n","#         disc_interpolates = netD.forward(interpolates)\n","\n","#         gradients = autograd.grad(\n","#             outputs=disc_interpolates, inputs=interpolates, grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n","#             create_graph=True, retain_graph=True, only_inputs=True\n","#         )[0]\n","\n","#         gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * self.LAMBDA\n","#         return gradient_penalty\n","\n","#     def get_loss( net, realA, fakeB, realB):\n","#         D_fake = net.forward(fakeB)\n","#         D_fake = D_fake.mean()\n","\n","#         # Real\n","#         D_real = net.forward(realB)\n","#         D_real = D_real.mean()\n","#         # Combined loss\n","#         loss_D = D_fake - D_real\n","#         gradient_penalty = calc_gradient_penalty(net, realB.data, fakeB.data)\n","#         return loss_D + gradient_penalty\n","\n","\t\t"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f9giIwy1bd6r"},"source":["#***Init Weights***"]},{"cell_type":"code","metadata":{"id":"0adDk39PcOpD"},"source":["def weights_init(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        m.weight.data.normal_(0.0, 0.02)\n","        if hasattr(m.bias, 'data'):\n","            m.bias.data.fill_(0)\n","    elif classname.find('BatchNorm2d') != -1:\n","        m.weight.data.normal_(1.0, 0.02)\n","        m.bias.data.fill_(0)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O3Nz3Iu4MAMq"},"source":["#***Initialize Model***"]},{"cell_type":"code","metadata":{"id":"CzBbEkU0bN3c","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cdc07802-bba7-4785-e5f7-aff33c172653"},"source":["channel_rate = 64\n","# Note the image_shape must be multiple of patch_shape\n","image_shape = (320, 320, 1)\n","patch_shape = (channel_rate, channel_rate, 3)\n","\n","ngf = 32\n","ndf = 32\n","input_nc = 1\n","output_nc = 1\n","\n","use_dropout=True\n","gpu_ids=[0]\n","use_parallel=False\n","learn_residual=False\n","use_sigmoid= True\n","\n","\n","norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=True)\n","netG = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=12,\n","                               gpu_ids=gpu_ids, use_parallel=use_parallel, learn_residual=learn_residual)\n","netG.cuda(gpu_ids[0])\n","# netG.apply(weights_init)\n","\n","n_layers_D=12\n","netD = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer, use_sigmoid=use_sigmoid,\n","                                   gpu_ids=gpu_ids, use_parallel=use_parallel)\n","netD.cuda(gpu_ids[0])\n","netD.apply(weights_init)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NLayerDiscriminator(\n","  (model): Sequential(\n","    (0): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n","    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n","    (3): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n","    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n","    (6): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n","    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (8): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n","    (9): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n","    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (11): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n","    (12): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n","    (13): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (14): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n","    (15): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n","    (16): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (17): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n","    (18): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n","    (19): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (20): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n","    (21): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n","    (22): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (23): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n","    (24): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n","    (25): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (26): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n","    (27): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n","    (28): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (29): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n","    (30): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n","    (31): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (32): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n","    (33): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n","    (34): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (35): Conv2d(256, 256, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n","    (36): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)\n","    (37): LeakyReLU(negative_slope=0.2, inplace=True)\n","    (38): Conv2d(256, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n","    (39): Sigmoid()\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"Xk-VyTOnwA-t"},"source":["# print(ConvBlock(1, 1, 0.0))\n","# print(ResnetBlock(1,1,\"reflect\",functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=True), True, nn.InstanceNorm2d ))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yr48nopGMrX_"},"source":["#***Load Data***"]},{"cell_type":"code","metadata":{"id":"7uqyjy-Rha7k"},"source":["class DataTransform:\n","    def __init__(self, mask_func, resolution, which_challenge, use_seed=True):\n","        self.resolution = resolution\n","        self.mask_func = mask_func\n","    def __call__(self, kspace, target, attrs, fname, slice):\n","        kspace = transforms.to_tensor(kspace)\n","        \n","        seed = tuple(map(ord, fname))\n","        masked_kspace, _ = transforms.apply_mask(kspace, self.mask_func, seed)\n","      \n","        image = transforms.ifft2(masked_kspace)a\n","        image = transforms.complex_center_crop(image, (self.resolution, self.resolution))\n","        image = transforms.complex_abs(image)\n","        image, mean, std = transforms.normalize_instance(image)\n","        image = image.clamp(-6, 6)\n","        target = transforms.to_tensor(target)\n","        # Normalize target\n","        target = transforms.normalize(target, mean, std, eps=1e-11)\n","        target = target.clamp(-6, 6)\n","        return image, target, mean, std\n","        #return image, mean, std, fname, slice"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LKqpJDoDhfb7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"683f1d01-67e6-41e8-bf05-783bd7ece3ad"},"source":["import pathlib\n","\n","root = pathlib.Path('/content/drive/MyDrive/Accelerated MRI Scanning/Repositories/fastMRImaster/Dataset/knee_sc/singlecoil_train/')\n","root_val=pathlib.Path('/content/drive/MyDrive/Accelerated MRI Scanning/Repositories/fastMRImaster/Dataset/knee_sc/singlecoil_val/')\n","batch_size=1\n","num_workers=8\n","\n","def create_dataset(root, resolution=320, center_fractions=[0.08], accelerations=[4]):\n","    \n","    mask_func = MaskFunc(center_fractions, accelerations)\n","    data = SliceData(\n","        root = root,\n","        transform = DataTransform(mask_func, resolution,'singlecoil'),\n","        sample_rate = 1.0,\n","        challenge = 'singlecoil'\n","    )\n","    return data\n","\n","data_train=create_dataset(root)\n","data_val=create_dataset(root_val)\n","    \n","def create_dataloader(data, batch_size, num_workers):\n","    dataloader = DataLoader(\n","        dataset = data,\n","        batch_size = batch_size,\n","        num_workers = num_workers,\n","    )\n","    return dataloader\n","\n","\n","data_load_train=create_dataloader(data_train,batch_size, num_workers)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"vTL4yziQN8ta"},"source":["#***Optimizer***"]},{"cell_type":"code","metadata":{"id":"yD7uwldnP_QV"},"source":["beta1=0.5\n","lr=0.001\n","device = torch.device('cuda') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kslyzuTdY2Mb"},"source":["\n","optimizer_G = torch.optim.Adam( netG.parameters(), lr=lr, betas=(beta1, 0.999) )\n","optimizer_D = torch.optim.Adam( netD.parameters(), lr=lr, betas=(beta1, 0.999) )\n","\n","def forward(netG,input_A,input_B):\n","\t\treal_A = Variable(input_A)\n","\t\tfake_B = netG.forward(real_A)\n","\t\treal_B = Variable(input_B)\n","\n","\n","def backward_D(netD, real_A, fake_B, real_B, eval=False):\n","  discLoss=DiscLoss()\n","  loss_D = discLoss.get_loss(netD, real_A, fake_B, real_B)\n","\n","  if not eval:\n","    #retaingraph=true save the gradients of prev batch(its in use as we are\n","    #calling backward() multiple times on same batch multiple times look optimize_parameters func )\n","    loss_D.backward(retain_graph=True)\n","\n","  return loss_D.item()\n","  \n","def backward_G(netD, real_A, fake_B, real_B,eval=False):\n","    discLoss=DiscLoss()\n","    # per_loss=PerceptualLoss(nn.MSELoss())\n","\n","    contentLoss = ContentLoss(nn.L1Loss())\n","    #print(fake_B.shape)\n","    #print(real_A.shape)\n","    loss_G_GAN = discLoss.get_g_loss(netD, real_A, fake_B)\n","    # Second, G(A) = B\n","    loss_G_Content = contentLoss.get_loss(fake_B, real_B) *100\n","    # loss_G_percept= per_loss.get_loss(fake_B, real_B)\n","    loss_G_percept = contextual_bilateral_loss(fake_B, real_B, loss_type = \"cosine\")\n","                                                                                                              # loss_G_percept,_ = CX_loss(real_B, fake_B)\n","                                                                                                              \n","    loss_G = loss_G_GAN + loss_G_Content +loss_G_percept*50\n","    \n","    if not eval:\n","      loss_G.backward()\n","    return loss_G.item()\n","    \n","def optimize_parameters(netD,netG, real_A, fake_B, real_B,optimizer_G,optimizer_D):\n","    forward(netG,real_A,real_B)\n","    criticUpdates=5\n","    lossd=[]\n","    for iter_d in range(criticUpdates):\n","        optimizer_D.zero_grad()\n","        lossd.append(backward_D(netD, real_A, fake_B, real_B))\n","        optimizer_D.step()\n","    loss_D=torch.mean(torch.tensor(lossd))\n","    optimizer_G.zero_grad()\n","    loss_G= backward_G(netD, real_A, fake_B, real_B)\n","    optimizer_G.step()\n","    \n","    return loss_G,loss_D"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OKUYlQQIPZZt"},"source":["#***PostProcessing***"]},{"cell_type":"code","metadata":{"id":"qSB2hyLZl2A5"},"source":["points = [\n","          [120.53896104 , 78.63419913], [201.05844156 , 78.63419913], [237.42207792 , 79.5       ], [ 81.57792208 , 76.03679654], [122.27056277 ,118.46103896], \n","          [199.32683983 ,116.72943723], [240.88528139 ,117.5952381 ], [198.46103896 ,199.84632035], [119.67316017 ,197.24891775], [ 82.44372294 ,196.38311688], \n","          [277.24891775 ,198.98051948], [201.05844156 ,237.94155844], [ 82.44372294 ,237.94155844], [ 39.15367965 ,120.19264069], [279.84632035 ,116.72943723],\n","          [274.65151515 , 80.36580087], [201.92424242 , 37.94155844], [117.07575758 , 41.4047619 ], [117.94155844 ,274.30519481], [197.5952381  ,281.23160173],\n","          [240.01948052 ,242.27056277], [278.98051948 ,241.4047619 ]\n","    ]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ta8TAdQ7mOGI"},"source":["import cv2\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import matplotlib\n","from math import exp, pow\n","\n","class IdealNotchFilter:\n","    def __init__(self):\n","        pass\n","    \n","    def apply_filter(self, fshift, points, d0):\n","        # print(fshift.shape)\n","        m = fshift.shape[0]\n","        n = fshift.shape[1]\n","        for u in range(m):\n","            for v in range(n):\n","                # print(u, v)\n","                for d in range(len(points)):\n","                    u0 = points[d][0]\n","                    v0 = points[d][1]\n","                    u0, v0 = v0, u0\n","                    d1 = pow(pow(u - u0, 2) + pow(v - v0, 2), 1)\n","                    d2 = pow(pow(u + u0, 2) + pow(v + v0, 2), 1)\n","                    if d1 <= d0 or d2 <= d0:\n","                        fshift[u][v] *= 0.0\n","        f_ishift = np.fft.ifftshift(fshift)\n","        img_back = np.fft.ifft2(f_ishift)\n","        img_back = np.abs(img_back)\n","        # plt.imshow(img_back)\n","        # plt.show()\n","        # matplotlib.image.imsave(path, img_back, cmap = \"gray\")\n","        return img_back\n","\n","class GaussianNotchFilter:\n","    def __init__(self):\n","        pass\n","    \n","    def apply_filter(self, fshift, points, d0):\n","        m = fshift.shape[0]\n","        n = fshift.shape[1]\n","        for u in range(m):\n","            for v in range(n):\n","                for d in range(len(points)):\n","                    u0 = points[d][0]\n","                    v0 = points[d][1]\n","                    u0, v0 = v0, u0\n","                    d1 = pow(pow(u - u0, 2) + pow(v - v0, 2), 0.5)\n","                    d2 = pow(pow(u + u0, 2) + pow(v + v0, 2), 0.5)\n","                    fshift[u][v] *= (1 - exp(-0.5 * (d1 * d2 / pow(d0, 2))))\n","\n","        f_ishift = np.fft.ifftshift(fshift)\n","        img_back = np.fft.ifft2(f_ishift)\n","        img_back = np.abs(img_back)\n","        # matplotlib.image.imsave(path, img_back, cmap = \"gray\")\n","        return img_back"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G3xaWCZtO52g"},"source":["#***Save or Load Model***"]},{"cell_type":"code","metadata":{"id":"Zc2Icg4edHO8"},"source":["weights_dir = \"/content/drive/MyDrive/Accelerated MRI Scanning/Repositories/FastMRI-Challenge/Phase 4/weights\"\n","#stores the weights in  weights folder\n","def save_network(network, network_label, epoch_label, gpu_ids):\n","  # return\n","  save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n","  save_path = os.path.join(weights_dir, save_filename)\n","  torch.save(network.cpu().state_dict(), save_path)\n","  if len(gpu_ids) and torch.cuda.is_available():\n","      network.cuda(device=gpu_ids[0])\n","\n","# run this before model initialization\n","def load_network(network, network_label, epoch_label):\n","  save_filename = '%s_net_%s.pth' % (epoch_label, network_label)\n","  save_path = os.path.join(weights_dir, save_filename)\n","  network.load_state_dict(torch.load(save_path))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0pCp8LGycs_l"},"source":["#***Main***"]},{"cell_type":"code","metadata":{"id":"105OSQjEob5d"},"source":["import random\n","import cv2\n","# from scipy.fft import fft, ifft\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Wb5660yoOhw"},"source":["\n","def generate_images(netG, test_input, tar, ep=-1, ep_iter=-1):\n","\n","  netG.eval()\n","  prediction = netG.forward(test_input)\n","  matplotlib.image.imsave(\"./Phase 4/images/Pred_\"+str(ep)+\"_\"+str(ep_iter)+\".png\", prediction[0,0].detach().cpu().numpy(), cmap = \"gray\")\n","\n","  # fun(tar)\n","  # fun(prediction, True)  \n","  \n","  # f = np.fft.fft2(prediction[0,0].detach().cpu().numpy())\n","  # fshift = np.fft.fftshift(f)\n","  # freq = 20 * np.log(np.abs(fshift))\n","  \n","  # tmp12 = IdealNotchFilter().apply_filter(fshift, points, 121.0)\n","\n","  plt.figure(figsize=(15,15))\n","  display_list = []\n","  display_list.append((test_input[0,0]).detach().cpu().numpy())\n","  display_list.append((tar[0,0]).detach().cpu().numpy())\n","  display_list.append((prediction[0,0]).detach().cpu().numpy())\n","  gt = display_list[1]\n","  pred = display_list[2]\n","  losses = {\"NMSE\":nmse(gt, pred),\"SSIM\": ssim(gt.reshape(1,320,320), pred.reshape(1,320,320)),\"PSNR\": psnr(gt, pred)}\n","\n","  print(losses, end = \"\\n\\n\") \n","  title = ['Input Image', 'Ground Truth', 'Predicted Image', 'Reconstruction Error']\n","  display_list.append((display_list[1]-display_list[2]))\n","\n","  for i in range(0, len(title)):\n","    plt.subplot(1, 4, i+1)\n","    plt.title(title[i])\n","    plt.imshow(display_list[i] * 0.5 + 0.5, cmap=\"gray\")\n","    plt.axis('off')\n","  if ep > -1 and ep_iter> -1:\n","    plt.savefig('./Phase 4/images/'+str(ep)+'_'+str(ep_iter)+'.png', dpi=300,  bbox_inches='tight')\n","  plt.show()\n","  netG.train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XsgBerpKMJEq"},"source":["report_interval=100"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OiLtlhrCNTIe"},"source":["def evaluate( epoch, netG, netD, data_loader,opt_g, opt_d, writer):\n","    netG.eval()\n","    losses = []\n","    start = time.perf_counter()\n","    with torch.no_grad():\n","        for iter, data in enumerate(data_loader):\n","            input, target, mean, std, norm = data\n","            input = input.unsqueeze(1).to(device)\n","            target = target.to(device)\n","            output = netG(input).squeeze(1)\n","\n","            mean = mean.unsqueeze(1).unsqueeze(2).to(device)\n","            std = std.unsqueeze(1).unsqueeze(2).to(device)\n","#            target = target * std + mean\n","#            output = output * std + mean\n","\n","#            norm = norm.unsqueeze(1).unsqueeze(2).to(args.device)\n","            loss = bakward_G(netD, input, output, target, eval= True)\n","            losses.append(loss.item())\n","        writer.add_scalar('Dev_Loss', np.mean(losses), epoch)\n","    return np.mean(losses), time.perf_counter() - start"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6kRO7BnKZfST"},"source":["device = torch.device('cuda') \n","epoch_count=5\n","batchSize=1\n","\n","# gloss = []\n","# dloss = []\n","# trainlo\n","import time\n","def train(data_loader, netG,netD, writer):\n","  #dataset = data_loader.load_data()\n","  dataset_size = len(data_loader)\n","  print('#training images = %d' % dataset_size)\n","  total_steps = 0\n","  \n","  train_time = 0\n","  for epoch in range(epoch_count):\n","    epoch_start_time = time.time()\n","    epoch_iter = 0\n","    start_epoch = start_iter = time.perf_counter()\n","    global_step = epoch * len(data_loader)\n","    print(enumerate(data_loader))\n","    for i, data in enumerate(data_loader):\n","      iter_start_time = time.time()\n","      total_steps += batchSize\n","      epoch_iter += batchSize\n","      input, target, mean, std = data\n","      \n","      #print(input.unsqueeze(1).shape)\n","      #input-input.expand(batchSize,320,320,1)\n","      #target=target.expand(batchSize,320,320,1)\n","      input = input.unsqueeze(1).to(device)\n","      #print(input.shape)\n","      target = target.unsqueeze(1).to(device)\n","      output = netG(input)#.squeeze(1)\n","      #print(target.shape)\n","      loss_g,loss_d=optimize_parameters(netD,netG,input,output,target,optimizer_G,optimizer_D)\n","      \n","      \n","      TrainLoss=loss_g+loss_d\n","      writer.add_scalar('TrainLoss',TrainLoss,global_step+i)\n","      avg_loss = 0.99 * avg_loss + 0.01 * TrainLoss if i > 0 else TrainLoss.item()\n","      writer.add_scalar('TrainLoss', avg_loss, global_step + i)\n","\n","      if i % report_interval == 0:\n","          logging.info(\n","              f'Epoch = [{epoch:3d}/{epoch_count:3d}] '\n","              f'Iter = [{i:4d}/{len(data_loader):4d}] '\n","              f'Loss = {TrainLoss:.4g} Avg Loss = {avg_loss:.4g} '\n","              f'Time = {time.perf_counter() - start_iter:.4f}s',\n","          )\n","        \n","      start_iter = time.perf_counter()\n","      train_time= time.perf_counter() - start_epoch\n","      \n","      print(f'Epoch = [{epoch:3d}/{epoch_count:3d}], ',f'Iter = [{i:4d}/{len(data_loader):4d}], ',f'Train Loss = {TrainLoss:.4g}, Avg Loss = {avg_loss:.4g}, ', f'Time = {time.perf_counter() - start_iter:.4f}s')\n","      \n","\n","      # if total_steps % report_interval == 0:\n","      if i% 50==0:\n","        # print('saving the latest model (epoch %d, total_steps %d)' % (epoch, total_steps))\n","        # save_network(netG, 'G', 'latest', gpu_ids) #gpu_ids daal dena\n","        # save_network(netD, 'D', 'latest', gpu_ids)\n","        print('Visualising images :- ')\n","        generate_images(netG, input, target, epoch, epoch_iter)\n","    \n","    \n","    e_loss , tme = evaluate(epoch, netG, netD, data_loader, optimizer_G, optimizer_D, writer)\n","    print(f\"\\n\\nLoss: {e_loss}, time: {tme}\")\n","    \n","    # if epoch % report_interval == 0:\n","    print('saving the model at the end of epoch %d, iters %d' % (epoch, total_steps))\n","    save_network(netG, 'G', 'latest', gpu_ids)\n","    save_network(netD, 'D', 'latest', gpu_ids)\n","    save_network(netG, 'G', epoch, gpu_ids)\n","    save_network(netD, 'D', epoch, gpu_ids)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SOzVDqg39d2R"},"source":["# !pip install torch==1.4.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XuKwKl_iGNd5"},"source":["torch.cuda.empty_cache()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A17GZtmNnCXd"},"source":["# LOAD_NETWORK\n","train(data_load_train,netG,netD, writer)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x6jSHm3cF1Pd"},"source":["a,b,c,d = [1,2,3,4]\n","print(a,b,c,d)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G6RoHERcahtg"},"source":["#***Testing***"]},{"cell_type":"code","metadata":{"id":"G9RDBxN8qb8X"},"source":["# import torch\n","\n","# noise = torch.distributions.Normal(torch.tensor([0.0]), torch.tensor(3.20))\n","# noise = noise.sample()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vrSVK1qeGM9B"},"source":["\n","# def generate_images(netG, test_input, tar):\n","#   # the training=True is intentional here since\n","#   # we want the batch statistics while running the netG\n","#   # on the test dataset. If we use training=False, we will get\n","#   # the accumulated statistics learned from the training dataset\n","#   # (which we don't want)\n","#   prediction = netG(test_input, training=True)\n","#   plt.figure(figsize=(15,15))\n","\n","#   display_list = [test_input[0], tar[0], prediction[0]]\n","#   title = ['Input Image', 'Ground Truth', 'Predicted Image']\n","\n","#   for i in range(3):\n","#     plt.subplot(1, 3, i+1)\n","#     plt.title(title[i])\n","#     # getting the pixel values between [0, 1] to plot it.\n","#     plt.imshow(display_list[i] * 0.5 + 0.5)\n","#     plt.axis('off')\n","#   plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tyx4hbgtRb5g"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pCvNU8xXRBMA"},"source":["import h5py"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"shfz8qwyQfJm"},"source":["flname =  pathlib.Path('/content/drive/MyDrive/Accelerated MRI Scanning/Repositories/fastMRImaster/Dataset/knee_sc/singlecoil_test/')\n","batch_size=1\n","num_workers=2\n","test_dataset = create_dataset(flname)\n","test_dataset_dataloader = create_dataloader(test_dataset,batch_size, num_workers)\n","\n","\n","# test_file = h5py.File(flname, \"r+\")\n","# test_kspace = test_file['reconstruction'][:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ImGLPrbgQ5_O"},"source":["# for i, x in enumerate(test_dataset_dataloader):\n","#   print(i, x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kesSnq-iUFmq"},"source":["netGload =  ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer, use_dropout=use_dropout, n_blocks=12, gpu_ids=gpu_ids, use_parallel=use_parallel, learn_residual=learn_residual)\n","netGload.cuda(gpu_ids[0])\n","# netGload.apply(weights_init)\n","load_network(netGload, \"G\", 4);"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dLE4D0aemo5Y"},"source":["data_load_val=create_dataloader(data_val,batch_size, num_workers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ydetPfAmmtvU"},"source":["device = torch.device('cuda')\n","for i, data in enumerate(data_load_val):\n","  if i % 20==0:\n","    input, target, mean, std = data\n","    input = input.unsqueeze(1).to(device)\n","    target = target.unsqueeze(1).to(device)\n","    print(input.shape, target.shape)\n","    generate_images(netGload, input, target, i, i)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oeWnleT3Rfwe"},"source":["Pre-processing "]},{"cell_type":"code","metadata":{"id":"3eOgE85yV1vj"},"source":["matplotlib.image.imsave('pre.png', sampled_image_abs4 , cmap=\"gray\")\n","\n","\n","#plt.savefig('foo.png')\n","\n","\n","# Open an already existing image\n","imageObject = Image.open(\"pre.png\");\n","imageObject.show();\n","\n","\n","# Apply sharp filter\n","sharpened1 = imageObject.filter(ImageFilter.SHARPEN);\n","sharpened2 = sharpened1.filter(ImageFilter.SHARPEN);\n","\n","# Show the sharpened images\n","\n","sharpened1.show();\n","sharpened2.show();\n","\n","plt.imshow(sharpened4)\n","\n","\n","show_slice(sharpened2 , cmap='gray')\n","\n","\n","t1=imageObject.filter(ImageFilter.EDGE_ENHANCE);\n","plt.imshow(t1)\n","\n","\n","t2=imageObject.filter(ImageFilter.EDGE_ENHANCE_MORE);\n","plt.imshow(t2)\n","\n","\n","t3=imageObject.filter(ImageFilter.SMOOTH_MORE);\n","plt.imshow(t3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4FodD1dQimMk"},"source":["#Three lines to make our compiler able to draw:\n","import sys\n","# matplotlib.use('Agg')\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","arr = np.ones((20,20))\n","\n","#Two  lines to make our compiler able to draw:\n","# plt.savefig(sys.stdout.buffer)\n","# sys.stdout.flush()\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"anvhaD12i4R6"},"source":["arr[6:14,6:14] = 3\n","arr[3:4] = 2\n","arr[19]=5\n","print(arr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Um-VgIKXjoWK"},"source":["\n","plt.plot(arr, \"k+\")\n","plt.show()\n"],"execution_count":null,"outputs":[]}]}